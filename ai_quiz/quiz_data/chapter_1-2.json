{
    "id": 3,
    "title": "챕터 1-2: MLP 구현 실습",
    "icon": "🧠",
    "questions": [
        {
            "difficulty": "중",
            "time": "1분",
            "question": "데이터 분할 시, 동일한 group_id를 가진 샘플이 훈련/검증집합에 중복되지 않도록 하려면 sklearn 클래스의 어떤 교차검증기를 사용해야 하나요?\n\n",
            "options": [
                "GroupKFold",
                "StratifiedKFold",
                "ShuffleSplit",
                "TimeSeriesSplit"
            ],
            "correct": 0,
            "explanation": "GroupKFold는 groups 파라미터를 받아 같은 그룹 인덱스가 훈련과 검증에 중복되지 않도록 분할합니다. StratifiedKFold는 클래스 비율을 유지하고, TimeSeriesSplit은 시계열 순서를 보장합니다."
        },
        {
            "difficulty": "중",
            "time": "3분",
            "question": "torch.utils.data.Dataset을 상속해 커스텀 데이터셋을 정의할 때 반드시 구현해야 하는 메서드는 무엇인가요?",
            "options": [
                "__open__, __close__",
                "__len__, __getitem__",
                "__iter__, __next__",
                "__load__, __preprocess__"
            ],
            "correct": 1,
            "explanation": "PyTorch의 Dataset 인터페이스는 __len__으로 전체 샘플 수를, __getitem__으로 인덱스별 샘플(데이터와 라벨)을 반환하도록 설계되어 있습니다. DataLoader는 이 두 메서드를 이용해 배치를 구성합니다. ⓵, ⓷, ⓸는 정의된 인터페이스가 아니며, 이터레이터(__iter__, __next__) 방식은 내부적으로 DataLoader가 처리하므로 직접 구현할 필요가 없습니다."
        },
        {
            "difficulty": "중",
            "time": "3분",
            "question": "입력 차원이 784, 은닉층 128, 출력층 10인 MLP가 있다. 이 네트워크의 학습 가능한 파라미터(가중치＋편향) 총 개수는?",
            "options": [
                "101,770",
                "100,480",
                "101,632",
                "99,842"
            ],
            "correct": 0,
            "explanation": "첫 번째 선형 계층: 가중치 784×128 = 100,352, 편향 128 → 100,480\n두 번째 선형 계층: 가중치 128×10 = 1,280, 편향 10 → 1,290\n총합 = 100,480 + 1,290 = 101,770"
        },
        {
            "difficulty": "하",
            "time": "3분",
            "question": "MLP 은닉층에서 사용 가능한 활성화 함수가 아닌 것은?",
            "options": [
                "ReLU",
                "Sigmoid",
                "Tanh",
                "Softmax"
            ],
            "correct": 3,
            "explanation": "⓵ ReLU, ⓶ Sigmoid, ⓷ Tanh는 은닉층에서 비선형성을 부여하기 위해 사용됩니다.\n⓸ Softmax는 주로 다중 클래스 분류의 출력층에서 클래스 확률을 계산할 때 사용하고, 은닉층에는 적용하지 않습니다."
        },
        {
            "difficulty": "상",
            "time": "3분",
            "question": "PyTorch의 nn.CrossEntropyLoss()가 내부적으로 수행하는 연산을 순서대로 고른 것은?",
            "options": [
                "Softmax → Log → NLLLoss",
                "Sigmoid → BinaryCrossEntropy",
                "Tanh → MSELoss",
                "Softmax → MSELoss"
            ],
            "correct": 0,
            "explanation": "CrossEntropyLoss는 먼저 로짓에 대해 로그-소프트맥스(log-softmax)를 적용하고, 그 결과에 음의 로그 우도(NLLLoss)를 계산합니다. 이 과정을 한 번에, 보다 수치적으로 안정적으로 처리하도록 설계된 손실함수입니다"
        }
    ]
}