{
    "id": 13,
    "title": "챕터 5-2(2): On-Device AI를 위한 모델 변환 및 실행",
    "icon": "��",
    "questions": [
        {
            "difficulty": "하",
            "time": "1분",
            "question": "ai-edge-torch 를 사용하여 PyTorch 모델을 TFLite로 변환하는 절차로 가장 올바른 것은 무엇인가?",
            "options": [
                "PyTorch 모델 로드 → ONNX 변환 → TFLite 변환 및 내보내기",
                "모델 재정의 및 그래프 추적 → TFLite 변환 및 양자화 → TFLite 파일로 내보내기",
                "모델 재정의 → 양자화 → 그래프 추적 → TFLite 변환 → TFLite 파일로 내보내기",
                "PyTorch 모델 로드 → TFLite Interpreter 실행 → 그래프 추적 및 내보내기"
            ],
            "correct": 1,
            "explanation": "⓵ ONNX를 중간 형식으로 사용하는 것은 일반적인 모델 변환 방법 중 하나이지만, ai-edge-torch의 핵심 장점은 이 중간 단계를 생략하고 PyTorch에서 TFLite로 직접 변환 경로를 제공하는 것 입니다.\n⓶ 먼저 모델을 재정의 한 후, 모델의 연산 그래프를 추적하고 TFLite 으로 변환 및 선택적 양자화를 수행한 후 TFLite 파일로 저장합니다.\n⓷ 절차의 순서가 올바르지 않습니다. 양자화는 모델의 연산 그래프를 추적한 뒤, TFLite 형식으로 변환하는 메서드 내에서 하나의 옵션으로 적용됩니다.\n⓸ 도구의 역할을 혼동한 절차입니다. TFLite Interpreter는 모델 변환 과정이 모두 끝난 후, 최종적으로 생성된 TFLite 파일을 실행하여 추론하는 데 사용되는 도구입니다. 모델을 변환하는 과정 자체에 사용되지 않습니다."
        },
        {
            "difficulty": "중",
            "time": "2분",
            "question": "이 실습에서 기존에 학습된 Hugging Face 모델을 그대로 사용하지 않고, ai-edge-torch 라이브러리를 통해 모델을 새롭게 정의(재구성)하는 가장 핵심적인 이유는 무엇인가?",
            "options": [
                "기존 모델의 성능이 낮아, 더 많은 레이어를 추가하여 정확도를 높이기 위해",
                "Hugging Face transformers 라이브러리가 시퀀스 분류 작업을 지원하지 않기 때문에",
                "TFLite 변환 및 모바일 하드웨어 가속을 보장하기 위해, 온디바이스 추론에 최적화된 전용 레이어로 모델을 다시 조립해야 하기 때문에",
                "PyTorch로 작성된 모델을 TensorFlow 코드로 완전히 새로 작성해야 변환이 가능하기 때문에"
            ],
            "correct": 2,
            "explanation": "⓵ 모델 재구성의 목표는 성능 향상보다는 최적화에 있습니다. 즉, 모델의 크기를 줄이고, 엣지 디바이스에서 추론 속도를 높이는 것이 주된 목적이며, 레이어를 추가하는 것과는 방향이 다릅니다.\n⓶ Hugging Face transformers 라이브러리는 시퀀스 분류를 위한 강력한 기능을 이미 제공하고 있습니다.\n⓷ Hugging Face의 표준 레이어들은 유연성이 높은 대신, 일부 연산이 TFLite가 요구하는 정적 그래프로 변환하기 어렵거나 모바일 NPU/GPU에서의 가속을 지원하지 않을 수 있습니다. 때문에 TFLite 변환이 보장된 최적화된 레이어로 모델을 다시 구성하는 과정이 필요합니다.\n⓸ ai-edge-torch 의 장점은 PyTorch 코드 자체를 그대로 유지하면서 TFLite로 변환할 수 있게 해주는 것입니다. TensorFlow 코드로 새로 작성할 필요 없이, PyTorch 생태계 안에서 온디바이스 배포를 준비할 수 있습니다."
        },
        {
            "difficulty": "상",
            "time": "3분",
            "question": "딥러닝 모델을 모바일이나 엣지 디바이스에 배포할 때, QKV 프로젝션을 하나로 합치는 것과 같이 여러 개의 작은 연산자(operator)를 하나의 큰 연산자로 통합하는 '연산자 융합(Operator Fusion)' 최적화 기법을 사용하는 주된 이유로 가장 적절한 것은 무엇인가?",
            "options": [
                "모델 그래프 내에서 입력값과 무관하게 항상 동일한 결과를 내는 부분을 컴파일 시점에 미리 계산하여 최종값으로 대체하고, 연산을 줄이기 위하여",
                "모델의 전체 파라미터 수나 연산량(FLOPs) 자체를 줄여, 모델을 근본적으로 경량화하기 위해",
                "메모리에서 데이터를 읽고 연산 커널(kernel)을 여러 번 호출하는 데 발생하는 오버헤드를 줄여, GPU/NPU와 같은 병렬 처리 장치의 효율을 극대화하기 위해",
                "Transformer의 반복적인 레이어 구조를 명시적으로 펼쳐 루프 제어에 따른 분기(branch)를 제거하고 명령어 파이프라인 효율을 높이기 위해"
            ],
            "correct": 2,
            "explanation": "⓵ 이 보기는 상수 폴딩(Constant Folding)이라는 다른 최적화 기법에 대한 설명입니다. 상수 폴딩은 모델의 입력값과 관계없이 항상 결과가 같은 부분을 컴파일 시점에 미리 계산해버리는 기술입니다. 예를 들어, 그래프 내에 5 * 10이라는 연산이 있다면, 이를 50이라는 상수 값으로 대체하여 런타임 시에는 연산을 아예 수행하지 않도록 합니다. 하지만 연산자 융합은 QKV 프로젝션처럼 입력에 따라 결과가 바뀌는 연산들을 합치는 것이므로, 상수 폴딩과는 다릅니다.\n⓶ 이 보기는 가지치기(Pruning)나 지식 증류(Knowledge Distillation)와 같은 모델 압축 기법에 대한 설명입니다. 이러한 기법들은 모델의 파라미터나 전체 연산량(FLOPs) 자체를 줄여 모델을 더 작고 가볍게 만듭니다. 반면, 연산자 융합은 연산량 자체를 줄이지는 않습니다. 예를 들어, 3개의 행렬 곱셈을 하나로 합쳐도 총 곱셈과 덧셈의 횟수는 동일합니다. 단지 연산을 수행하는 방식을 효율적으로 바꿀 뿐입니다.\n⓷ 이것이 연산자 융합(Operator Fusion)의 정확한 목적입니다. GPU나 NPU 같은 병렬 처리 장치는 연산을 실행할 때마다 커널 호출 오버헤드나 메모리 접근 오버헤드가 발생합니다. 작은 연산을 여러 번 실행하면 이 오버헤드가 계속 누적됩니다. 연산자 융합은 여러 연산을 하나의 큰 연산으로 묶어, 커널 호출과 메모리 접근 횟수를 단 한 번으로 줄여줍니다. 따라서 실제 계산량은 같아도 전체적인 추론 속도는 빨라집니다.\n⓸ 이 보기는 루프 언롤링(Loop Unrolling)이라는 컴파일러 최적화 기법에 대한 설명입니다. 루프 언롤링은 for문과 같이 반복되는 구조를 명시적으로 풀어서 나열함으로써, 루프를 제어하는 데 필요한 조건 검사나 분기(branch) 명령어를 제거하는 기술입니다. 이는 주로 CPU의 명령어 파이프라인 효율을 높이는 데 목적이 있습니다. 연산자 융합은 루프 구조가 아닌, 순차적으로 나열된 서로 다른 연산들을 합치는 것이므로 루프 언롤링과는 다른 개념입니다."
        },
        {
            "difficulty": "하",
            "time": "1분",
            "question": "TFLite Interpreter 의 역할에 대한 설명으로 가장 적절한것은?",
            "options": [
                "PyTorch 모델을 실시간으로 TFLite 형식으로 변환하는 JIT 컴파일러이다.",
                "모바일 기기에서 PyTorch 모델의 학습을 가능하게 해주는 도구이다.",
                "미리 변환된 TFLite 모델 파일을 불러와, 입력 데이터를 받아 추론(Inference)을 실행하는 엔진이다.",
                "모델을 불러오기 직전에 자동으로 양자화를 수행하여 모델을 최적화한다."
            ],
            "correct": 2,
            "explanation": "⓵ 모델 변환은 사전에(offline) 한 번만 수행됩니다. Interpreter는 실시간 변환이 아닌, 이미 변환된 모델의 실행을 담당합니다.\n⓶ TFLite Interpreter는 추론에 특화되어 있습니다.\n⓷ Interpreter의 핵심 역할은 최적화 및 변환이 완료된 TFLite 파일을 메모리에 로드하고 입력에 대한 예측 결과를 계산하는 것 입니다.\n⓸ 양자화는 모델 변환 시점에 적용됩니다. Interpreter 가 실행 시점에 양자화를 수행하지는 않습니다."
        },
        {
            "difficulty": "중",
            "time": "2분",
            "question": "TFLite 변환 과정에서 더미 입력(dummy input)을 사용하여 그래프 추적(Graph Tracing)을 수행하는 이유는 무엇인가요?",
            "options": [
                "모델의 가중치를 무작위 값으로 초기화하여 처음부터 다시 학습시키기 위해",
                "입력값에 따라 연산 흐름이 변하는 동적 요소를 제거하고, 모든 연산의 순서와 종류가 고정된 정적 그래프 형태로 만들기 위해",
                "더미 입력에 대한 모델의 출력값을 미리 계산하여 추론 속도를 높이는 캐싱(caching)을 하기 위해",
                "입력 텐서의 차원에 맞춰 모델의 레이어 수를 동적으로 조절하기 위해"
            ],
            "correct": 1,
            "explanation": "⓵ 그래프 추적은 모델의 구조를 파악하는 과정이며, 이미 학습된 가중치를 변경하지 않습니다.\n⓶ TFLite와 같은 온디바이스 런타임은 효율적인 실행을 위해 연산의 흐름이 변하지 않는 정적 그래프(Static Graph)를 필요로 합니다. 더미 입력을 모델에 통과시켜 실제 어떤 연산들이 어떤 순서로 실행되는지 기록(추적)하고, 이 흐름을 그대로 고정하여 TFLite 모델로 만듭니다.\n⓷ 더미 입력은 모델의 연산 경로를 확인하는 용도로만 한 번 사용될 뿐, 그 결과가 저장되어 재사용되지는 않습니다.\n⓸ 그래프 추적의 목적은 동적인 요소를 제거하고 구조를 고정하는 것입니다. 입력을 기반으로 모델 구조를 바꾸는 것은 정적 그래프의 원칙에 위배됩니다."
        }
    ]
}